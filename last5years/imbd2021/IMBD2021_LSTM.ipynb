{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b624wv0l6CMW"
      },
      "source": [
        "LSTM程式碼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tCZFGH-uLVf",
        "outputId": "d26d6dae-b331-4635-d992-0a94f1d397c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_11100/3967151030.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  train = pd.concat([train, train_temp[tar_cols][:]], ignore_index=True)\n",
            "/tmp/ipykernel_11100/3967151030.py:57: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  test = pd.concat([test, test2_temp[tar_cols][:]], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch is 1 \tTraining Loss: 933.474403\n",
            "Epoch is 2 \tTraining Loss: 925.345158\n",
            "Epoch is 3 \tTraining Loss: 919.319438\n",
            "Epoch is 4 \tTraining Loss: 914.310235\n",
            "Epoch is 5 \tTraining Loss: 909.298260\n",
            "Epoch is 6 \tTraining Loss: 904.030059\n",
            "Epoch is 7 \tTraining Loss: 898.304963\n",
            "Epoch is 8 \tTraining Loss: 892.274021\n",
            "Epoch is 9 \tTraining Loss: 886.629601\n",
            "Epoch is 10 \tTraining Loss: 880.752546\n",
            "Epoch is 11 \tTraining Loss: 874.231123\n",
            "Epoch is 12 \tTraining Loss: 866.624632\n",
            "Epoch is 13 \tTraining Loss: 859.191866\n",
            "Epoch is 14 \tTraining Loss: 851.461414\n",
            "Epoch is 15 \tTraining Loss: 842.963003\n",
            "Epoch is 16 \tTraining Loss: 834.680826\n",
            "Epoch is 17 \tTraining Loss: 827.244088\n",
            "Epoch is 18 \tTraining Loss: 819.918437\n",
            "Epoch is 19 \tTraining Loss: 812.580786\n",
            "Epoch is 20 \tTraining Loss: 805.116241\n",
            "Epoch is 21 \tTraining Loss: 797.556471\n",
            "Epoch is 22 \tTraining Loss: 789.744350\n",
            "Epoch is 23 \tTraining Loss: 781.935869\n",
            "Epoch is 24 \tTraining Loss: 773.888010\n",
            "Epoch is 25 \tTraining Loss: 765.615988\n",
            "Epoch is 26 \tTraining Loss: 757.176760\n",
            "Epoch is 27 \tTraining Loss: 748.682805\n",
            "Epoch is 28 \tTraining Loss: 740.082609\n",
            "Epoch is 29 \tTraining Loss: 731.270610\n",
            "Epoch is 30 \tTraining Loss: 722.089889\n",
            "Epoch is 31 \tTraining Loss: 712.518738\n",
            "Epoch is 32 \tTraining Loss: 703.012053\n",
            "Epoch is 33 \tTraining Loss: 693.240032\n",
            "Epoch is 34 \tTraining Loss: 683.730360\n",
            "Epoch is 35 \tTraining Loss: 674.513241\n",
            "Epoch is 36 \tTraining Loss: 665.176081\n",
            "Epoch is 37 \tTraining Loss: 655.616976\n",
            "Epoch is 38 \tTraining Loss: 645.950036\n",
            "Epoch is 39 \tTraining Loss: 636.122808\n",
            "Epoch is 40 \tTraining Loss: 626.483455\n",
            "Epoch is 41 \tTraining Loss: 616.815058\n",
            "Epoch is 42 \tTraining Loss: 607.295782\n",
            "Epoch is 43 \tTraining Loss: 597.498264\n",
            "Epoch is 44 \tTraining Loss: 588.005004\n",
            "Epoch is 45 \tTraining Loss: 578.255059\n",
            "Epoch is 46 \tTraining Loss: 568.632380\n",
            "Epoch is 47 \tTraining Loss: 559.079980\n",
            "Epoch is 48 \tTraining Loss: 549.329852\n",
            "Epoch is 49 \tTraining Loss: 539.500155\n",
            "Epoch is 50 \tTraining Loss: 530.114230\n",
            "Epoch is 51 \tTraining Loss: 520.466486\n",
            "Epoch is 52 \tTraining Loss: 510.754999\n",
            "Epoch is 53 \tTraining Loss: 501.278439\n",
            "Epoch is 54 \tTraining Loss: 491.792661\n",
            "Epoch is 55 \tTraining Loss: 482.169691\n",
            "Epoch is 56 \tTraining Loss: 472.776171\n",
            "Epoch is 57 \tTraining Loss: 463.292122\n",
            "Epoch is 58 \tTraining Loss: 454.014541\n",
            "Epoch is 59 \tTraining Loss: 444.547355\n",
            "Epoch is 60 \tTraining Loss: 435.399192\n",
            "Epoch is 61 \tTraining Loss: 426.012468\n",
            "Epoch is 62 \tTraining Loss: 416.556127\n",
            "Epoch is 63 \tTraining Loss: 407.122084\n",
            "Epoch is 64 \tTraining Loss: 397.615578\n",
            "Epoch is 65 \tTraining Loss: 388.268557\n",
            "Epoch is 66 \tTraining Loss: 378.867312\n",
            "Epoch is 67 \tTraining Loss: 369.978467\n",
            "Epoch is 68 \tTraining Loss: 360.718784\n",
            "Epoch is 69 \tTraining Loss: 351.931448\n",
            "Epoch is 70 \tTraining Loss: 343.172114\n",
            "Epoch is 71 \tTraining Loss: 334.283274\n",
            "Epoch is 72 \tTraining Loss: 325.604178\n",
            "Epoch is 73 \tTraining Loss: 317.115602\n",
            "Epoch is 74 \tTraining Loss: 308.782542\n",
            "Epoch is 75 \tTraining Loss: 300.146604\n",
            "Epoch is 76 \tTraining Loss: 291.993088\n",
            "Epoch is 77 \tTraining Loss: 283.562608\n",
            "Epoch is 78 \tTraining Loss: 275.461113\n",
            "Epoch is 79 \tTraining Loss: 267.497348\n",
            "Epoch is 80 \tTraining Loss: 259.663051\n",
            "Epoch is 81 \tTraining Loss: 252.020666\n",
            "Epoch is 82 \tTraining Loss: 244.176679\n",
            "Epoch is 83 \tTraining Loss: 236.701829\n",
            "Epoch is 84 \tTraining Loss: 229.047679\n",
            "Epoch is 85 \tTraining Loss: 221.623556\n",
            "Epoch is 86 \tTraining Loss: 214.549365\n",
            "Epoch is 87 \tTraining Loss: 207.187855\n",
            "Epoch is 88 \tTraining Loss: 200.436038\n",
            "Epoch is 89 \tTraining Loss: 193.563071\n",
            "Epoch is 90 \tTraining Loss: 186.851511\n",
            "Epoch is 91 \tTraining Loss: 180.530117\n",
            "Epoch is 92 \tTraining Loss: 173.801974\n",
            "Epoch is 93 \tTraining Loss: 167.445813\n",
            "Epoch is 94 \tTraining Loss: 161.068498\n",
            "Epoch is 95 \tTraining Loss: 154.959106\n",
            "Epoch is 96 \tTraining Loss: 149.193067\n",
            "Epoch is 97 \tTraining Loss: 143.547357\n",
            "Epoch is 98 \tTraining Loss: 137.585392\n",
            "Epoch is 99 \tTraining Loss: 132.124768\n",
            "Epoch is 100 \tTraining Loss: 126.734253\n",
            "Epoch is 101 \tTraining Loss: 121.450310\n",
            "Epoch is 102 \tTraining Loss: 116.241646\n",
            "Epoch is 103 \tTraining Loss: 111.603299\n",
            "Epoch is 104 \tTraining Loss: 106.956365\n",
            "Epoch is 105 \tTraining Loss: 102.500614\n",
            "Epoch is 106 \tTraining Loss: 97.575631\n",
            "Epoch is 107 \tTraining Loss: 93.388290\n",
            "Epoch is 108 \tTraining Loss: 89.121582\n",
            "Epoch is 109 \tTraining Loss: 84.953923\n",
            "Epoch is 110 \tTraining Loss: 81.278617\n",
            "Epoch is 111 \tTraining Loss: 77.192653\n",
            "Epoch is 112 \tTraining Loss: 74.015585\n",
            "Epoch is 113 \tTraining Loss: 70.170992\n",
            "Epoch is 114 \tTraining Loss: 65.644825\n",
            "Epoch is 115 \tTraining Loss: 61.416767\n",
            "Epoch is 116 \tTraining Loss: 57.080605\n",
            "Epoch is 117 \tTraining Loss: 53.501734\n",
            "Epoch is 118 \tTraining Loss: 50.579633\n",
            "Epoch is 119 \tTraining Loss: 47.262745\n",
            "Epoch is 120 \tTraining Loss: 44.537203\n",
            "Epoch is 121 \tTraining Loss: 41.750543\n",
            "Epoch is 122 \tTraining Loss: 39.195388\n",
            "Epoch is 123 \tTraining Loss: 36.898021\n",
            "Epoch is 124 \tTraining Loss: 34.442090\n",
            "Epoch is 125 \tTraining Loss: 32.230916\n",
            "Epoch is 126 \tTraining Loss: 30.451540\n",
            "Epoch is 127 \tTraining Loss: 28.586204\n",
            "Epoch is 128 \tTraining Loss: 26.781261\n",
            "Epoch is 129 \tTraining Loss: 25.200372\n",
            "Epoch is 130 \tTraining Loss: 23.446032\n",
            "Epoch is 131 \tTraining Loss: 22.204337\n",
            "Epoch is 132 \tTraining Loss: 21.019275\n",
            "Epoch is 133 \tTraining Loss: 19.603878\n",
            "Epoch is 134 \tTraining Loss: 18.827711\n",
            "Epoch is 135 \tTraining Loss: 17.692602\n",
            "Epoch is 136 \tTraining Loss: 16.615855\n",
            "Epoch is 137 \tTraining Loss: 15.723169\n",
            "Epoch is 138 \tTraining Loss: 14.733369\n",
            "Epoch is 139 \tTraining Loss: 14.349255\n",
            "Epoch is 140 \tTraining Loss: 13.534291\n",
            "Epoch is 141 \tTraining Loss: 13.045227\n",
            "Epoch is 142 \tTraining Loss: 12.573294\n",
            "Epoch is 143 \tTraining Loss: 12.238852\n",
            "Epoch is 144 \tTraining Loss: 11.546720\n",
            "Epoch is 145 \tTraining Loss: 10.991346\n",
            "Epoch is 146 \tTraining Loss: 10.662987\n",
            "Epoch is 147 \tTraining Loss: 10.490818\n",
            "Epoch is 148 \tTraining Loss: 10.279345\n",
            "Epoch is 149 \tTraining Loss: 9.860851\n",
            "Epoch is 150 \tTraining Loss: 9.856094\n",
            "Epoch is 151 \tTraining Loss: 9.787119\n",
            "Epoch is 152 \tTraining Loss: 9.574195\n",
            "Epoch is 153 \tTraining Loss: 9.405644\n",
            "Epoch is 154 \tTraining Loss: 9.305442\n",
            "Epoch is 155 \tTraining Loss: 9.453764\n",
            "Epoch is 156 \tTraining Loss: 9.093711\n",
            "Epoch is 157 \tTraining Loss: 9.192332\n",
            "Epoch is 158 \tTraining Loss: 9.191602\n",
            "Epoch is 159 \tTraining Loss: 9.135467\n",
            "Epoch is 160 \tTraining Loss: 9.014267\n",
            "Epoch is 161 \tTraining Loss: 9.062748\n",
            "Epoch is 162 \tTraining Loss: 8.942664\n",
            "Epoch is 163 \tTraining Loss: 8.787289\n",
            "Epoch is 164 \tTraining Loss: 9.025330\n",
            "Epoch is 165 \tTraining Loss: 8.925846\n",
            "Epoch is 166 \tTraining Loss: 8.726521\n",
            "Epoch is 167 \tTraining Loss: 8.684197\n",
            "Epoch is 168 \tTraining Loss: 8.723139\n",
            "Epoch is 169 \tTraining Loss: 8.718386\n",
            "Epoch is 170 \tTraining Loss: 8.625625\n",
            "Epoch is 171 \tTraining Loss: 8.704058\n",
            "Epoch is 172 \tTraining Loss: 8.466052\n",
            "Epoch is 173 \tTraining Loss: 8.531495\n",
            "Epoch is 174 \tTraining Loss: 8.548227\n",
            "Epoch is 175 \tTraining Loss: 8.510849\n",
            "Epoch is 176 \tTraining Loss: 8.376465\n",
            "Epoch is 177 \tTraining Loss: 8.336874\n",
            "Epoch is 178 \tTraining Loss: 8.612998\n",
            "Epoch is 179 \tTraining Loss: 8.164941\n",
            "Epoch is 180 \tTraining Loss: 8.427974\n",
            "Epoch is 181 \tTraining Loss: 8.306051\n",
            "Epoch is 182 \tTraining Loss: 8.368655\n",
            "Epoch is 183 \tTraining Loss: 8.270832\n",
            "Epoch is 184 \tTraining Loss: 8.152499\n",
            "Epoch is 185 \tTraining Loss: 8.277868\n",
            "Epoch is 186 \tTraining Loss: 8.085720\n",
            "Epoch is 187 \tTraining Loss: 8.216023\n",
            "Epoch is 188 \tTraining Loss: 8.027200\n",
            "Epoch is 189 \tTraining Loss: 8.102837\n",
            "Epoch is 190 \tTraining Loss: 8.205128\n",
            "Epoch is 191 \tTraining Loss: 7.947332\n",
            "Epoch is 192 \tTraining Loss: 7.975660\n",
            "Epoch is 193 \tTraining Loss: 7.905907\n",
            "Epoch is 194 \tTraining Loss: 7.991047\n",
            "Epoch is 195 \tTraining Loss: 8.329962\n",
            "Epoch is 196 \tTraining Loss: 7.890298\n",
            "Epoch is 197 \tTraining Loss: 7.974356\n",
            "Epoch is 198 \tTraining Loss: 7.823555\n",
            "Epoch is 199 \tTraining Loss: 7.753405\n",
            "Epoch is 200 \tTraining Loss: 7.875120\n",
            "Epoch is 201 \tTraining Loss: 7.710492\n",
            "Epoch is 202 \tTraining Loss: 7.747886\n",
            "Epoch is 203 \tTraining Loss: 7.715664\n",
            "Epoch is 204 \tTraining Loss: 7.701750\n",
            "Epoch is 205 \tTraining Loss: 7.758222\n",
            "Epoch is 206 \tTraining Loss: 7.983142\n",
            "Epoch is 207 \tTraining Loss: 7.681689\n",
            "Epoch is 208 \tTraining Loss: 7.563453\n",
            "Epoch is 209 \tTraining Loss: 7.743771\n",
            "Epoch is 210 \tTraining Loss: 7.549052\n",
            "Epoch is 211 \tTraining Loss: 7.608048\n",
            "Epoch is 212 \tTraining Loss: 7.960561\n",
            "Epoch is 213 \tTraining Loss: 7.534150\n",
            "Epoch is 214 \tTraining Loss: 7.627712\n",
            "Epoch is 215 \tTraining Loss: 7.414807\n",
            "Epoch is 216 \tTraining Loss: 7.370062\n",
            "Epoch is 217 \tTraining Loss: 7.319493\n",
            "Epoch is 218 \tTraining Loss: 7.549940\n",
            "Epoch is 219 \tTraining Loss: 7.386647\n",
            "Epoch is 220 \tTraining Loss: 7.527087\n",
            "Epoch is 221 \tTraining Loss: 7.272634\n",
            "Epoch is 222 \tTraining Loss: 7.440238\n",
            "Epoch is 223 \tTraining Loss: 7.431041\n",
            "Epoch is 224 \tTraining Loss: 7.463106\n",
            "Epoch is 225 \tTraining Loss: 7.206523\n",
            "Epoch is 226 \tTraining Loss: 7.258898\n",
            "Epoch is 227 \tTraining Loss: 7.388890\n",
            "Epoch is 228 \tTraining Loss: 7.383376\n",
            "Epoch is 229 \tTraining Loss: 7.472174\n",
            "Epoch is 230 \tTraining Loss: 7.310008\n",
            "Epoch is 231 \tTraining Loss: 7.190838\n",
            "Epoch is 232 \tTraining Loss: 7.231906\n",
            "Epoch is 233 \tTraining Loss: 7.380709\n",
            "Epoch is 234 \tTraining Loss: 7.230133\n",
            "Epoch is 235 \tTraining Loss: 7.115796\n",
            "Epoch is 236 \tTraining Loss: 7.219716\n",
            "Epoch is 237 \tTraining Loss: 7.276528\n",
            "Epoch is 238 \tTraining Loss: 7.192709\n",
            "Epoch is 239 \tTraining Loss: 7.210643\n",
            "Epoch is 240 \tTraining Loss: 7.286603\n",
            "Epoch is 241 \tTraining Loss: 7.137213\n",
            "Epoch is 242 \tTraining Loss: 7.137214\n",
            "Epoch is 243 \tTraining Loss: 7.292343\n",
            "Epoch is 244 \tTraining Loss: 7.195812\n",
            "Epoch is 245 \tTraining Loss: 7.114664\n",
            "Epoch is 246 \tTraining Loss: 7.067303\n",
            "Epoch is 247 \tTraining Loss: 7.058154\n",
            "Epoch is 248 \tTraining Loss: 7.018072\n",
            "Epoch is 249 \tTraining Loss: 6.998859\n",
            "Epoch is 250 \tTraining Loss: 7.329443\n",
            "Epoch is 251 \tTraining Loss: 7.211347\n",
            "Epoch is 252 \tTraining Loss: 7.006718\n",
            "Epoch is 253 \tTraining Loss: 7.151351\n",
            "Epoch is 254 \tTraining Loss: 7.087012\n",
            "Epoch is 255 \tTraining Loss: 7.302094\n",
            "Epoch is 256 \tTraining Loss: 7.160328\n",
            "Epoch is 257 \tTraining Loss: 6.877662\n",
            "Epoch is 258 \tTraining Loss: 6.958775\n",
            "Epoch is 259 \tTraining Loss: 6.897194\n",
            "Epoch is 260 \tTraining Loss: 6.924691\n",
            "Epoch is 261 \tTraining Loss: 7.077547\n",
            "Epoch is 262 \tTraining Loss: 6.839079\n",
            "Epoch is 263 \tTraining Loss: 6.791762\n",
            "Epoch is 264 \tTraining Loss: 6.866751\n",
            "Epoch is 265 \tTraining Loss: 6.853686\n",
            "Epoch is 266 \tTraining Loss: 7.191714\n",
            "Epoch is 267 \tTraining Loss: 7.078226\n",
            "Epoch is 268 \tTraining Loss: 6.911781\n",
            "Epoch is 269 \tTraining Loss: 6.806461\n",
            "Epoch is 270 \tTraining Loss: 6.776350\n",
            "Epoch is 271 \tTraining Loss: 6.824125\n",
            "Epoch is 272 \tTraining Loss: 6.902150\n",
            "Epoch is 273 \tTraining Loss: 6.772516\n",
            "Epoch is 274 \tTraining Loss: 6.913939\n",
            "Epoch is 275 \tTraining Loss: 6.826351\n",
            "Epoch is 276 \tTraining Loss: 6.646258\n",
            "Epoch is 277 \tTraining Loss: 6.657264\n",
            "Epoch is 278 \tTraining Loss: 6.967040\n",
            "Epoch is 279 \tTraining Loss: 6.781948\n",
            "Epoch is 280 \tTraining Loss: 6.611892\n",
            "Epoch is 281 \tTraining Loss: 6.537703\n",
            "Epoch is 282 \tTraining Loss: 6.827829\n",
            "Epoch is 283 \tTraining Loss: 6.567498\n",
            "Epoch is 284 \tTraining Loss: 6.669359\n",
            "Epoch is 285 \tTraining Loss: 7.048282\n",
            "Epoch is 286 \tTraining Loss: 6.628244\n",
            "Epoch is 287 \tTraining Loss: 6.573528\n",
            "Epoch is 288 \tTraining Loss: 6.608967\n",
            "Epoch is 289 \tTraining Loss: 6.640945\n",
            "Epoch is 290 \tTraining Loss: 6.613895\n",
            "Epoch is 291 \tTraining Loss: 6.566864\n",
            "Epoch is 292 \tTraining Loss: 6.730847\n",
            "Epoch is 293 \tTraining Loss: 6.576992\n",
            "Epoch is 294 \tTraining Loss: 6.625156\n",
            "Epoch is 295 \tTraining Loss: 6.649161\n",
            "Epoch is 296 \tTraining Loss: 6.492766\n",
            "Epoch is 297 \tTraining Loss: 6.478652\n",
            "Epoch is 298 \tTraining Loss: 6.517828\n",
            "Epoch is 299 \tTraining Loss: 6.619489\n",
            "Epoch is 300 \tTraining Loss: 6.479852\n",
            "Epoch is 301 \tTraining Loss: 6.673445\n",
            "Epoch is 302 \tTraining Loss: 6.497844\n",
            "Epoch is 303 \tTraining Loss: 6.665106\n",
            "Epoch is 304 \tTraining Loss: 6.496682\n",
            "Epoch is 305 \tTraining Loss: 6.505726\n",
            "Epoch is 306 \tTraining Loss: 6.505084\n",
            "Epoch is 307 \tTraining Loss: 6.407631\n",
            "Epoch is 308 \tTraining Loss: 6.356905\n",
            "Epoch is 309 \tTraining Loss: 6.681217\n",
            "Epoch is 310 \tTraining Loss: 6.630939\n",
            "Epoch is 311 \tTraining Loss: 6.511439\n",
            "Epoch is 312 \tTraining Loss: 6.480131\n",
            "Epoch is 313 \tTraining Loss: 6.387487\n",
            "Epoch is 314 \tTraining Loss: 6.274956\n",
            "Epoch is 315 \tTraining Loss: 6.458476\n",
            "Epoch is 316 \tTraining Loss: 6.703831\n",
            "Epoch is 317 \tTraining Loss: 6.349740\n",
            "Epoch is 318 \tTraining Loss: 6.360668\n",
            "Epoch is 319 \tTraining Loss: 6.447429\n",
            "Epoch is 320 \tTraining Loss: 6.377427\n",
            "Epoch is 321 \tTraining Loss: 6.422101\n",
            "Epoch is 322 \tTraining Loss: 6.202269\n",
            "Epoch is 323 \tTraining Loss: 6.248830\n",
            "Epoch is 324 \tTraining Loss: 6.215504\n",
            "Epoch is 325 \tTraining Loss: 6.202846\n",
            "Epoch is 326 \tTraining Loss: 6.277449\n",
            "Epoch is 327 \tTraining Loss: 6.162226\n",
            "Epoch is 328 \tTraining Loss: 6.313701\n",
            "Epoch is 329 \tTraining Loss: 6.289414\n",
            "Epoch is 330 \tTraining Loss: 6.297368\n",
            "Epoch is 331 \tTraining Loss: 6.512851\n",
            "Epoch is 332 \tTraining Loss: 6.236137\n",
            "Epoch is 333 \tTraining Loss: 6.365317\n",
            "Epoch is 334 \tTraining Loss: 6.212114\n",
            "Epoch is 335 \tTraining Loss: 6.269596\n",
            "Epoch is 336 \tTraining Loss: 6.104451\n",
            "Epoch is 337 \tTraining Loss: 6.452357\n",
            "Epoch is 338 \tTraining Loss: 6.192880\n",
            "Epoch is 339 \tTraining Loss: 6.145274\n",
            "Epoch is 340 \tTraining Loss: 6.165225\n",
            "Epoch is 341 \tTraining Loss: 6.281378\n",
            "Epoch is 342 \tTraining Loss: 6.279227\n",
            "Epoch is 343 \tTraining Loss: 6.093453\n",
            "Epoch is 344 \tTraining Loss: 6.133672\n",
            "Epoch is 345 \tTraining Loss: 6.144829\n",
            "Epoch is 346 \tTraining Loss: 6.070910\n",
            "Epoch is 347 \tTraining Loss: 5.989209\n",
            "Epoch is 348 \tTraining Loss: 6.139007\n",
            "Epoch is 349 \tTraining Loss: 6.189582\n",
            "Epoch is 350 \tTraining Loss: 6.039739\n",
            "Epoch is 351 \tTraining Loss: 6.035718\n",
            "Epoch is 352 \tTraining Loss: 6.154540\n",
            "Epoch is 353 \tTraining Loss: 5.984212\n",
            "Epoch is 354 \tTraining Loss: 6.027309\n",
            "Epoch is 355 \tTraining Loss: 5.898012\n",
            "Epoch is 356 \tTraining Loss: 6.069645\n",
            "Epoch is 357 \tTraining Loss: 5.935829\n",
            "Epoch is 358 \tTraining Loss: 6.179645\n",
            "Epoch is 359 \tTraining Loss: 6.063480\n",
            "Epoch is 360 \tTraining Loss: 5.956420\n",
            "Epoch is 361 \tTraining Loss: 5.914777\n",
            "Epoch is 362 \tTraining Loss: 5.819376\n",
            "Epoch is 363 \tTraining Loss: 5.697561\n",
            "Epoch is 364 \tTraining Loss: 5.937450\n",
            "Epoch is 365 \tTraining Loss: 5.955998\n",
            "Epoch is 366 \tTraining Loss: 5.966705\n",
            "Epoch is 367 \tTraining Loss: 5.783708\n",
            "Epoch is 368 \tTraining Loss: 5.925658\n",
            "Epoch is 369 \tTraining Loss: 5.933882\n",
            "Epoch is 370 \tTraining Loss: 5.871559\n",
            "Epoch is 371 \tTraining Loss: 6.129213\n",
            "Epoch is 372 \tTraining Loss: 6.120358\n",
            "Epoch is 373 \tTraining Loss: 5.935385\n",
            "Epoch is 374 \tTraining Loss: 5.871186\n",
            "Epoch is 375 \tTraining Loss: 5.999646\n",
            "Epoch is 376 \tTraining Loss: 5.996531\n",
            "Epoch is 377 \tTraining Loss: 5.820355\n",
            "Epoch is 378 \tTraining Loss: 5.897006\n",
            "Epoch is 379 \tTraining Loss: 5.984621\n",
            "Epoch is 380 \tTraining Loss: 5.828280\n",
            "Epoch is 381 \tTraining Loss: 5.942055\n",
            "Epoch is 382 \tTraining Loss: 5.716182\n",
            "Epoch is 383 \tTraining Loss: 5.850972\n",
            "Epoch is 384 \tTraining Loss: 5.693411\n",
            "Epoch is 385 \tTraining Loss: 5.800613\n",
            "Epoch is 386 \tTraining Loss: 5.991067\n",
            "Epoch is 387 \tTraining Loss: 5.848319\n",
            "Epoch is 388 \tTraining Loss: 5.623721\n",
            "Epoch is 389 \tTraining Loss: 5.722386\n",
            "Epoch is 390 \tTraining Loss: 5.604129\n",
            "Epoch is 391 \tTraining Loss: 5.795243\n",
            "Epoch is 392 \tTraining Loss: 5.496210\n",
            "Epoch is 393 \tTraining Loss: 5.735592\n",
            "Epoch is 394 \tTraining Loss: 5.622309\n",
            "Epoch is 395 \tTraining Loss: 5.771545\n",
            "Epoch is 396 \tTraining Loss: 5.707166\n",
            "Epoch is 397 \tTraining Loss: 5.763744\n",
            "Epoch is 398 \tTraining Loss: 5.569720\n",
            "Epoch is 399 \tTraining Loss: 5.730626\n",
            "Epoch is 400 \tTraining Loss: 5.660469\n",
            "Epoch is 401 \tTraining Loss: 5.667879\n",
            "Epoch is 402 \tTraining Loss: 5.603344\n",
            "Epoch is 403 \tTraining Loss: 5.794098\n",
            "Epoch is 404 \tTraining Loss: 5.639692\n",
            "Epoch is 405 \tTraining Loss: 5.538486\n",
            "Epoch is 406 \tTraining Loss: 5.572523\n",
            "Epoch is 407 \tTraining Loss: 5.587743\n",
            "Epoch is 408 \tTraining Loss: 5.748685\n",
            "Epoch is 409 \tTraining Loss: 5.531916\n",
            "Epoch is 410 \tTraining Loss: 5.695471\n",
            "Epoch is 411 \tTraining Loss: 5.566633\n",
            "Epoch is 412 \tTraining Loss: 5.531157\n",
            "Epoch is 413 \tTraining Loss: 5.570039\n",
            "Epoch is 414 \tTraining Loss: 5.622810\n",
            "Epoch is 415 \tTraining Loss: 5.597452\n",
            "Epoch is 416 \tTraining Loss: 5.758377\n",
            "Epoch is 417 \tTraining Loss: 5.683870\n",
            "Epoch is 418 \tTraining Loss: 5.483246\n",
            "Epoch is 419 \tTraining Loss: 5.480092\n",
            "Epoch is 420 \tTraining Loss: 5.607722\n",
            "Epoch is 421 \tTraining Loss: 5.622492\n",
            "Epoch is 422 \tTraining Loss: 5.560481\n",
            "Epoch is 423 \tTraining Loss: 5.541473\n",
            "Epoch is 424 \tTraining Loss: 5.497122\n",
            "Epoch is 425 \tTraining Loss: 5.534679\n",
            "Epoch is 426 \tTraining Loss: 5.586098\n",
            "Epoch is 427 \tTraining Loss: 5.477461\n",
            "Epoch is 428 \tTraining Loss: 5.640463\n",
            "Epoch is 429 \tTraining Loss: 5.571337\n",
            "Epoch is 430 \tTraining Loss: 5.522658\n",
            "Epoch is 431 \tTraining Loss: 5.652921\n",
            "Epoch is 432 \tTraining Loss: 5.746634\n",
            "Epoch is 433 \tTraining Loss: 5.518105\n",
            "Epoch is 434 \tTraining Loss: 5.461346\n",
            "Epoch is 435 \tTraining Loss: 5.479459\n",
            "Epoch is 436 \tTraining Loss: 5.386844\n",
            "Epoch is 437 \tTraining Loss: 5.570484\n",
            "Epoch is 438 \tTraining Loss: 5.387165\n",
            "Epoch is 439 \tTraining Loss: 5.613601\n",
            "Epoch is 440 \tTraining Loss: 5.446943\n",
            "Epoch is 441 \tTraining Loss: 5.413440\n",
            "Epoch is 442 \tTraining Loss: 5.388020\n",
            "Epoch is 443 \tTraining Loss: 5.580815\n",
            "Epoch is 444 \tTraining Loss: 5.385589\n",
            "Epoch is 445 \tTraining Loss: 5.403488\n",
            "Epoch is 446 \tTraining Loss: 5.436215\n",
            "Epoch is 447 \tTraining Loss: 5.448039\n",
            "Epoch is 448 \tTraining Loss: 5.359328\n",
            "Epoch is 449 \tTraining Loss: 5.594124\n",
            "Epoch is 450 \tTraining Loss: 5.304851\n",
            "Epoch is 451 \tTraining Loss: 5.470865\n",
            "Epoch is 452 \tTraining Loss: 5.462910\n",
            "Epoch is 453 \tTraining Loss: 5.387577\n",
            "Epoch is 454 \tTraining Loss: 5.480707\n",
            "Epoch is 455 \tTraining Loss: 5.483514\n",
            "Epoch is 456 \tTraining Loss: 5.565437\n",
            "Epoch is 457 \tTraining Loss: 5.321139\n",
            "Epoch is 458 \tTraining Loss: 5.411864\n",
            "Epoch is 459 \tTraining Loss: 5.323774\n",
            "Epoch is 460 \tTraining Loss: 5.254952\n",
            "Epoch is 461 \tTraining Loss: 5.198882\n",
            "Epoch is 462 \tTraining Loss: 5.321920\n",
            "Epoch is 463 \tTraining Loss: 5.412749\n",
            "Epoch is 464 \tTraining Loss: 5.124289\n",
            "Epoch is 465 \tTraining Loss: 5.423939\n",
            "Epoch is 466 \tTraining Loss: 5.182098\n",
            "Epoch is 467 \tTraining Loss: 5.374174\n",
            "Epoch is 468 \tTraining Loss: 5.324529\n",
            "Epoch is 469 \tTraining Loss: 5.428897\n",
            "Epoch is 470 \tTraining Loss: 5.327017\n",
            "Epoch is 471 \tTraining Loss: 5.437480\n",
            "Epoch is 472 \tTraining Loss: 5.134739\n",
            "Epoch is 473 \tTraining Loss: 5.273248\n",
            "Epoch is 474 \tTraining Loss: 5.232545\n",
            "Epoch is 475 \tTraining Loss: 5.169879\n",
            "Epoch is 476 \tTraining Loss: 5.181030\n",
            "Epoch is 477 \tTraining Loss: 5.273583\n",
            "Epoch is 478 \tTraining Loss: 5.222484\n",
            "Epoch is 479 \tTraining Loss: 5.169491\n",
            "Epoch is 480 \tTraining Loss: 5.167047\n",
            "Epoch is 481 \tTraining Loss: 5.307187\n",
            "Epoch is 482 \tTraining Loss: 5.147217\n",
            "Epoch is 483 \tTraining Loss: 5.264721\n",
            "Epoch is 484 \tTraining Loss: 5.185108\n",
            "Epoch is 485 \tTraining Loss: 5.148669\n",
            "Epoch is 486 \tTraining Loss: 5.148760\n",
            "Epoch is 487 \tTraining Loss: 5.052150\n",
            "Epoch is 488 \tTraining Loss: 5.225581\n",
            "Epoch is 489 \tTraining Loss: 5.084333\n",
            "Epoch is 490 \tTraining Loss: 5.112203\n",
            "Epoch is 491 \tTraining Loss: 5.219340\n",
            "Epoch is 492 \tTraining Loss: 5.089228\n",
            "Epoch is 493 \tTraining Loss: 5.207448\n",
            "Epoch is 494 \tTraining Loss: 5.144035\n",
            "Epoch is 495 \tTraining Loss: 5.279848\n",
            "Epoch is 496 \tTraining Loss: 5.280738\n",
            "Epoch is 497 \tTraining Loss: 5.106610\n",
            "Epoch is 498 \tTraining Loss: 5.268281\n",
            "Epoch is 499 \tTraining Loss: 5.115590\n",
            "Epoch is 500 \tTraining Loss: 5.107610\n",
            "mae= 5.1925616\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "step = 30  # 時序步數\n",
        "\n",
        "train_temp = pd.read_csv('./data/train.csv')\n",
        "test2_temp = pd.read_csv('./data/test.csv')\n",
        "\n",
        "#%%\n",
        "#依照訓練集數據長度建立空的資料結構\n",
        "n_data = train_temp.shape[0]\n",
        "time_step = step\n",
        "data = np.zeros((n_data, 12, time_step), dtype='float32')\n",
        "\n",
        "row_start = 0\n",
        "row_end = 0\n",
        "tar_cols = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11', 'F12']\n",
        "\n",
        "#依照設定的步數處理數據\n",
        "row_end = len(train_temp)\n",
        "for j in range(time_step):\n",
        "    zero_cols = time_step - j - 1\n",
        "    train = pd.DataFrame([[0] * len(tar_cols)] * zero_cols, columns=tar_cols)\n",
        "    if zero_cols == 0:\n",
        "        train = pd.concat([train, train_temp[tar_cols][:]], ignore_index=True)\n",
        "    else:\n",
        "        train = pd.concat([train, train_temp[tar_cols][:-zero_cols]], ignore_index=True)\n",
        "    data[row_start:row_end, :, j] = train\n",
        "\n",
        "tar_cols = ['O']\n",
        "data_y = np.zeros((n_data, 1), dtype='float32')\n",
        "row_start = 0\n",
        "row_end = len(train_temp)\n",
        "data_y[row_start:row_end, :] = train_temp[tar_cols]\n",
        "\n",
        "#%%\n",
        "#依照測試集數據長度建立空的資料結構\n",
        "n_data = test2_temp.shape[0]\n",
        "tar_cols = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11', 'F12']\n",
        "\n",
        "time_step = step\n",
        "test2_data = np.zeros((n_data, 12, time_step), dtype='float32')\n",
        "row_start = 0\n",
        "row_end = 0\n",
        "#依照設定的步數處理數據\n",
        "row_end = len(test2_temp)\n",
        "for j in range(time_step):\n",
        "    zero_cols = time_step - j - 1\n",
        "    test = pd.DataFrame([[0] * len(tar_cols)] * zero_cols, columns=tar_cols)\n",
        "    if zero_cols == 0:\n",
        "        test = pd.concat([test, test2_temp[tar_cols][:]], ignore_index=True)\n",
        "    else:\n",
        "        test = pd.concat([test, test2_temp[tar_cols][:-zero_cols]], ignore_index=True)\n",
        "    test2_data[row_start:row_end, :, j] = test\n",
        "\n",
        "tar_cols = ['O']\n",
        "test2_y = np.zeros((n_data, 1), dtype='float32')\n",
        "row_start = 0\n",
        "row_end = len(test2_temp)\n",
        "test2_y[row_start:row_end, :] = test2_temp[tar_cols]\n",
        "\n",
        "#%%\n",
        "#建立數據集張量與定義網路\n",
        "train_data = torch.from_numpy(data)\n",
        "train_y = torch.from_numpy(data_y)\n",
        "torch_dataset = Data.TensorDataset(train_data, train_y)\n",
        "\n",
        "test2_data = torch.from_numpy(test2_data)\n",
        "test2_y = torch.from_numpy(test2_y)\n",
        "\n",
        "train_loader = Data.DataLoader(\n",
        "    dataset=torch_dataset,     \n",
        "    batch_size=1024,      \n",
        "    shuffle=True,               \n",
        "    num_workers=0,             \n",
        ")\n",
        "\n",
        "#%%\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #input_size = 特徵量\n",
        "        #hidden_size = 節點數\n",
        "        #num_layers = 隱藏層數量\n",
        "        self.rnn = nn.LSTM(input_size=12, hidden_size=16, num_layers=1, batch_first=False, bidirectional=True)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(16, 16)\n",
        "        \n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        \n",
        "        self.bn3 = nn.BatchNorm1d(16)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(16, 16)\n",
        "        \n",
        "        self.bn4 = nn.BatchNorm1d(16)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(16, 16)\n",
        "        \n",
        "        self.bn5 = nn.BatchNorm1d(16)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        \n",
        "        self.fc5 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs, (ht, ct) = self.rnn(x)\n",
        "        \n",
        "        x = ht[-1]\n",
        "        residual = x\n",
        "        \n",
        "        x = self.fc1(self.relu1(self.bn1(x)))\n",
        "        x = self.fc2(self.relu2(self.bn2(x)))\n",
        "        x += residual\n",
        "        \n",
        "        residual = x\n",
        "        x = self.fc3(self.relu3(self.bn3(x)))\n",
        "        x = self.fc4(self.relu4(self.bn4(x)))\n",
        "        x += residual\n",
        "        \n",
        "        x = self.fc5(self.relu5(self.bn5(x)))\n",
        "        return x\n",
        "\n",
        "#%%\n",
        "#訓練\n",
        "net = Net()\n",
        "epoch_iter = 500  # 1000\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "learning_rate = 1e-4\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=list(range(epoch_iter, epoch_iter, 12)), gamma=0.1)\n",
        "\n",
        "pths_path = './model/'\n",
        "\n",
        "for epoch in range(epoch_iter):\n",
        "    net.train()\n",
        "    epoch_loss = 0\n",
        "    val_loss = 0\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x = x.permute(2, 0, 1)\n",
        "        pred_y = net(x)\n",
        "        loss1 = loss_fn(y[:, 0], pred_y[:, 0])\n",
        "        loss = loss1\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    net.eval()\n",
        "    train_loss = epoch_loss / ((i + 1) * 2)\n",
        "    print('Epoch is {} \\tTraining Loss: {:.6f}'.format(epoch + 1, train_loss))\n",
        "\n",
        "state_dict = net.state_dict()\n",
        "torch.save(state_dict, os.path.join(pths_path, 'model_step' + str(step) + '.pth'))\n",
        "\n",
        "#%%\n",
        "#測試\n",
        "net = Net()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "pths_path = './model/'\n",
        "net.load_state_dict(torch.load(os.path.join(pths_path, 'model_step' + str(step) + '.pth')))\n",
        "net.eval()\n",
        "\n",
        "x = test2_data.to(device)\n",
        "x = x.permute(2, 0, 1)\n",
        "pred_y = net(x)\n",
        "pred_y = pred_y.to('cpu')\n",
        "\n",
        "loss_fn(pred_y, test2_y)\n",
        "loss_fn(pred_y[:, 0], test2_y[:, 0])\n",
        "\n",
        "pred_y = pred_y.detach().numpy()\n",
        "\n",
        "mae = mean_absolute_error(test2_y, pred_y)\n",
        "print('mae=', mae)  # 8.607\n",
        "resultPath = './pred_result/'\n",
        "np.savetxt(resultPath + 'testdata_Step' + str(step) + '.csv', pred_y, delimiter=',')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
